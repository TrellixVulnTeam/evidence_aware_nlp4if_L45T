[17-06-2020 17:53:10] INFO: Namespace(batch_size=32, bert_hidden_dim=768, bert_pretrain='../bert_base', checkpoint='../checkpoint/pretrain/pointwise/model.best.pt', cuda=True, dropout=0.6, evi_num=5, layer=1, max_len=120, name='dev.json', no_cuda=False, num_labels=2, outdir='./output/', test_path='../data/all_dev.json', threshold=0.0)
[17-06-2020 17:53:10] INFO: Start training!
[17-06-2020 17:53:10] INFO: loading vocabulary file ../bert_base/vocab.txt
[17-06-2020 17:53:10] INFO: loading training set
[17-06-2020 17:53:11] INFO: initializing estimator model
[17-06-2020 17:53:11] INFO: loading archive file ../bert_base
[17-06-2020 17:53:11] INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

[17-06-2020 17:53:12] INFO: Weights from pretrained model not used in BertForSequenceEncoder: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
[17-06-2020 17:53:14] INFO: Start eval!
Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.
Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.
